# TOPS Performance Measurement

Simple tool to measure AI performance in **TOPS** (Tera Operations Per Second) across different hardware configurations.

## Quick Start

### 1. Install Dependencies
```bash
# Option 1: Install individual packages
pip install torch torchvision numpy psutil py-cpuinfo pynvml

# Option 2: Install from requirements file
pip install -r tops_requirements.txt
```

### 2. Run TOPS Measurement
```bash
# Full benchmark
python tops_benchmark.py

# Quick benchmark (faster)
python tops_benchmark.py --quick

# Save to specific file
python tops_benchmark.py --output my_tops_results.json
```

### 3. View Results
The script will output results like this:
```
üíª SYSTEM INFORMATION
==================================================
Platform: Linux-5.4.0-84-generic-x86_64-with-glibc2.31
CPU: Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz
Cores: 8 physical, 16 logical
Frequency: 4800 MHz (max)
RAM: 32.0 GB total, 28.5 GB available
GPU(s):
  - NVIDIA GeForce RTX 3080 (10.0 GB)
CUDA: 11.8 (PyTorch)
Compute Device: cuda

üöÄ RUNNING TOPS BENCHMARKS
==================================================
NumPy TOPS: 0.45
Matrix Multiply TOPS: 12.34
Conv2D TOPS: 8.67
Linear TOPS: 15.23
ResNet Inference TOPS: 6.89
==================================================
üìä PERFORMANCE SUMMARY
==================================================
üèÜ PEAK PERFORMANCE: 15.23 TOPS
üìä AVERAGE PERFORMANCE: 8.72 TOPS
üíª Device: cuda
ü•á Best Operation: Linear (15.23 TOPS)
```

## What Gets Measured

| Operation | Description | Use Case |
|-----------|-------------|----------|
| **Matrix Multiply** | Large matrix multiplication | Core neural network operation |
| **Conv2D** | 2D Convolution | Image processing, CNNs |
| **Linear** | Dense/Fully-connected layers | Classification, transformers |
| **ResNet Inference** | Real model inference | Practical AI workload |
| **NumPy** | CPU baseline | Reference performance |

## GitHub Actions

The workflow automatically measures TOPS across:
- **Ubuntu** (with CUDA if available)
- **Windows** (with CUDA if available) 
- **macOS** (CPU/MPS)

Results are posted as PR comments showing performance comparison.

## Expected Performance Ranges

| Hardware Type | Typical TOPS Range |
|---------------|-------------------|
| **CPU (Intel/AMD)** | 0.1 - 2 TOPS |
| **Apple M1/M2** | 2 - 8 TOPS |
| **RTX 3060** | 15 - 25 TOPS |
| **RTX 4090** | 50 - 100 TOPS |
| **A100** | 100 - 300 TOPS |
| **GitHub Runners** | 0.5 - 3 TOPS |

## Command Line Options

```bash
python tops_benchmark.py --help

options:
  --output OUTPUT    Output JSON file (default: tops_results.json)
  --quick           Run quick benchmark with fewer iterations
```

## Results File Format

```json
{
  "timestamp": "2025-01-09T12:00:00.000000",
  "device": "cuda",
  "peak_tops": 15.23,
  "average_tops": 8.72,
  "system_info": {
    "platform": "Linux-5.4.0-84-generic-x86_64-with-glibc2.31",
    "cpu": {
      "brand": "Intel(R) Core(TM) i7-10700K CPU @ 3.80GHz",
      "physical_cores": 8,
      "logical_cores": 16,
      "max_frequency_mhz": 4800
    },
    "memory": {
      "total_gb": 32.0,
      "available_gb": 28.5
    },
    "gpu": {
      "detected_gpus": [
        {
          "name": "NVIDIA GeForce RTX 3080",
          "memory_gb": 10.0,
          "type": "NVIDIA"
        }
      ],
      "pytorch_cuda": {
        "available": true,
        "cuda_version": "11.8"
      }
    }
  },
  "tops_measurements": {
    "matrix_multiply": {
      "operation": "Matrix Multiply",
      "tops": 12.34,
      "avg_time_seconds": 0.164,
      "total_ops": 17179869184
    },
    "conv2d": {
      "operation": "Conv2D", 
      "tops": 8.67,
      "batch_size": 32,
      "avg_time_seconds": 0.089
    }
  }
}
```

## Hardware Requirements

- **Python 3.9+**
- **NumPy** (always required)
- **PyTorch** (for GPU acceleration)
- **psutil** (for system information)
- **py-cpuinfo** (for detailed CPU information - optional)
- **pynvml** (for NVIDIA GPU information - optional)

### GPU Support
- **NVIDIA**: Install CUDA toolkit + PyTorch with CUDA
- **Apple Silicon**: PyTorch with MPS support
- **CPU-only**: Works with any system

## Troubleshooting

**CUDA Out of Memory:**
```bash
python tops_benchmark.py --quick
```

**No GPU Detected:**
- Check `nvidia-smi` (NVIDIA)
- Verify PyTorch installation: `python -c "import torch; print(torch.cuda.is_available())"`

**Slow Performance:**
- Use `--quick` flag for faster results
- Check if other processes are using GPU/CPU

## What TOPS Means

**TOPS** = Tera (10¬π¬≤) Operations Per Second

For AI workloads, this typically means:
- **Multiply-accumulate operations** in matrix math
- **Convolution operations** in image processing
- **Element-wise operations** in neural networks

Higher TOPS = faster AI inference and training.